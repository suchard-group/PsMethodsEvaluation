@article{austinIntroductionPropensityScore2011,
  title = {An {{Introduction}} to {{Propensity Score Methods}} for {{Reducing}} the {{Effects}} of {{Confounding}} in {{Observational Studies}}.},
  author = {Austin, Peter C.},
  year = {2011},
  month = may,
  journal = {Multivariate behavioral research},
  volume = {46},
  number = {3},
  pages = {399--424},
  address = {United States},
  issn = {1532-7906 0027-3171},
  doi = {10.1080/00273171.2011.568786},
  abstract = {The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and  analyze an observational (nonrandomized) study so that it mimics some of the  particular characteristics of a randomized controlled trial. In particular, the  propensity score is a balancing score: conditional on the propensity score, the  distribution of observed baseline covariates will be similar between treated and  untreated subjects. I describe 4 different propensity score methods: matching on  the propensity score, stratification on the propensity score, inverse probability  of treatment weighting using the propensity score, and covariate adjustment using  the propensity score. I describe balance diagnostics for examining whether the  propensity score model has been adequately specified. Furthermore, I discuss  differences between regression-based methods and propensity score-based methods  for the analysis of observational data. I describe different causal average  treatment effects and their relationship with propensity score analyses.},
  langid = {english},
  pmcid = {PMC3144483},
  pmid = {21818162}
}

@article{austinOptimalCaliperWidths2011,
  title = {Optimal Caliper Widths for Propensity-Score Matching When Estimating Differences in Means and Differences in Proportions in Observational Studies},
  author = {Austin, Peter C.},
  year = {2011},
  month = mar,
  journal = {Pharmaceutical Statistics},
  volume = {10},
  number = {2},
  pages = {150--161},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1539-1604},
  doi = {10.1002/pst.433},
  urldate = {2024-12-18},
  abstract = {Abstract In a study comparing the effects of two treatments, the propensity score is the probability of assignment to one treatment conditional on a subject's measured baseline covariates. Propensity-score matching is increasingly being used to estimate the effects of exposures using observational data. In the most common implementation of propensity-score matching, pairs of treated and untreated subjects are formed whose propensity scores differ by at most a pre-specified amount (the caliper width). There has been a little research into the optimal caliper width. We conducted an extensive series of Monte Carlo simulations to determine the optimal caliper width for estimating differences in means (for continuous outcomes) and risk differences (for binary outcomes). When estimating differences in means or risk differences, we recommend that researchers match on the logit of the propensity score using calipers of width equal to 0.2 of the standard deviation of the logit of the propensity score. When at least some of the covariates were continuous, then either this value, or one close to it, minimized the mean square error of the resultant estimated treatment effect. It also eliminated at least 98\% of the bias in the crude estimator, and it resulted in confidence intervals with approximately the correct coverage rates. Furthermore, the empirical type I error rate was approximately correct. When all of the covariates were binary, then the choice of caliper width had a much smaller impact on the performance of estimation of risk differences and differences in means. Copyright ? 2010 John Wiley \& Sons, Ltd.},
  keywords = {bias,binary data,matching,Monte Carlo simulations,observational study,propensity score,propensity-score matching,risk difference}
}

@article{cochranControllingBiasObservational1973,
  title = {Controlling {{Bias}} in {{Observational Studies}}: {{A Review}}},
  author = {Cochran, William G. and Rubin, Donald B.},
  year = {1973},
  journal = {Sankhy{\=a}: The Indian Journal of Statistics, Series A (1961-2002)},
  volume = {35},
  number = {4},
  eprint = {25049893},
  eprinttype = {jstor},
  pages = {417--446},
  publisher = {Springer},
  issn = {0581572X},
  urldate = {2024-12-18},
  abstract = {[This paper reviews work on the effectiveness of different methods of matched sampling and statistical adjustment, alone and in combination, in reducing bias due to confounding x-variables when comparing two populations. The adjustment methods were linear regression adjustment for x continuous and direct standardization for x categorical. With x continuous, the range of situations examined included linear relations between y and x, parallel and non-parallel, monotonic non-linear parallel relations, equal and unequal variances of x, and the presence of errors of measurement in x. The percent of initial bias \$E({\textbackslash}overline\{y\}\_\{1\}-{\textbackslash}overline\{y\}\_\{2\})\$ that was removed was used as the criterion. Overall, linear regression adjustment on random samples appeared superior to the matching methods, with linear regression adjustment on matched samples the most robust method. Several different approaches were suggested for the case of multivariate x, on which little or no work has been done.]}
}

@article{colomaReferenceStandardEvaluation2013,
  title = {A {{Reference Standard}} for {{Evaluation}} of {{Methods}} for {{Drug Safety Signal Detection Using Electronic Healthcare Record Databases}}},
  author = {Coloma, Preciosa M. and Avillach, Paul and Salvo, Francesco and Schuemie, Martijn J. and Ferrajolo, Carmen and Pariente, Antoine and {Fourrier-R{\'e}glat}, Annie and Molokhia, Mariam and Patadia, Vaishali and {van der Lei}, Johan and Sturkenboom, Miriam and Trifir{\`o}, Gianluca},
  year = {2013},
  month = jan,
  journal = {Drug Safety},
  volume = {36},
  number = {1},
  pages = {13--23},
  issn = {1179-1942},
  doi = {10.1007/s40264-012-0002-x},
  abstract = {The growing interest in using electronic healthcare record (EHR) databases for drug safety surveillance has spurred development of new methodologies for signal detection. Although several drugs have been withdrawn postmarketing by regulatory authorities after scientific evaluation of harms and benefits, there is no definitive list of confirmed signals (i.e. list of all known adverse reactions and which drugs can cause them). As there is no true gold standard, prospective evaluation of signal detection methods remains a challenge.}
}

@article{hernanUsingBigData2016a,
  title = {Using {{Big Data}} to {{Emulate}} a {{Target Trial When}} a {{Randomized Trial Is Not Available}}.},
  author = {Hern{\'a}n, Miguel A. and Robins, James M.},
  year = {2016},
  month = apr,
  journal = {American journal of epidemiology},
  volume = {183},
  number = {8},
  pages = {758--764},
  address = {United States},
  issn = {1476-6256 0002-9262},
  doi = {10.1093/aje/kwv254},
  abstract = {Ideally, questions about comparative effectiveness or safety would be answered using an appropriately designed and conducted randomized experiment. When we  cannot conduct a randomized experiment, we analyze observational data. Causal  inference from large observational databases (big data) can be viewed as an  attempt to emulate a randomized experiment-the target experiment or target  trial-that would answer the question of interest. When the goal is to guide  decisions among several strategies, causal analyses of observational data need to  be evaluated with respect to how well they emulate a particular target trial. We  outline a framework for comparative effectiveness research using big data that  makes the target trial explicit. This framework channels counterfactual theory  for comparing the effects of sustained treatment strategies, organizes analytic  approaches, provides a structured process for the criticism of observational  studies, and helps avoid common methodologic pitfalls.},
  copyright = {{\copyright} The Author 2016. Published by Oxford University Press on behalf of the Johns Hopkins Bloomberg School of Public Health. All rights reserved. For permissions,  please e-mail: journals.permissions@oup.com.},
  langid = {english},
  pmcid = {PMC4832051},
  pmid = {26994063},
  keywords = {big data,Breast Neoplasms/*epidemiology/etiology,causal inference,Causality,comparative effectiveness research,Comparative Effectiveness Research/methods/*standards,Databases Factual/statistics & numerical data,Drug Therapy Combination,Estrogen Replacement Therapy/adverse effects/*statistics & numerical data,Female,Humans,Observational Studies as Topic/methods/*standards,Postmenopause,Progestins/adverse effects/*therapeutic use,Propensity Score,Randomized Controlled Trials as Topic/methods/standards,target trial}
}

@article{hripcsakObservationalHealthData2015,
  title = {Observational {{Health Data Sciences}} and {{Informatics}} ({{OHDSI}}): {{Opportunities}} for {{Observational Researchers}}.},
  author = {Hripcsak, George and Duke, Jon D. and Shah, Nigam H. and Reich, Christian G. and Huser, Vojtech and Schuemie, Martijn J. and Suchard, Marc A. and Park, Rae Woong and Wong, Ian Chi Kei and Rijnbeek, Peter R. and {van der Lei}, Johan and Pratt, Nicole and Nor{\'e}n, G. Niklas and Li, Yu-Chuan and Stang, Paul E. and Madigan, David and Ryan, Patrick B.},
  year = {2015},
  journal = {Studies in health technology and informatics},
  volume = {216},
  pages = {574--578},
  address = {Netherlands},
  issn = {1879-8365 0926-9630},
  abstract = {The vision of creating accessible, reliable clinical evidence by accessing the clincial experience of hundreds of millions of patients across the globe is a  reality. Observational Health Data Sciences and Informatics (OHDSI) has built on  learnings from the Observational Medical Outcomes Partnership to turn methods  research and insights into a suite of applications and exploration tools that  move the field closer to the ultimate goal of generating evidence about all  aspects of healthcare to serve the needs of patients, clinicians and all other  decision-makers around the world.},
  langid = {english},
  pmcid = {PMC4815923},
  pmid = {26262116},
  keywords = {*Databases Factual,*Models Organizational,*Observational Studies as Topic,Health Services Research/*organization & administration,Internationality,Medical Informatics/*organization & administration}
}

@article{kheraLargescaleEvidenceGeneration2022,
  title = {Large-Scale Evidence Generation and Evaluation across a Network of Databases for Type 2 Diabetes Mellitus ({{LEGEND-T2DM}}): A Protocol for a Series of Multinational,  Real-World Comparative Cardiovascular Effectiveness and Safety Studies.},
  author = {Khera, Rohan and Schuemie, Martijn J. and Lu, Yuan and Ostropolets, Anna and Chen, RuiJun and Hripcsak, George and Ryan, Patrick B. and Krumholz, Harlan M. and Suchard, Marc A.},
  year = {2022},
  month = jun,
  journal = {BMJ open},
  volume = {12},
  number = {6},
  pages = {e057977},
  address = {England},
  issn = {2044-6055},
  doi = {10.1136/bmjopen-2021-057977},
  abstract = {INTRODUCTION: Therapeutic options for type 2 diabetes mellitus (T2DM) have expanded over the last decade with the emergence of cardioprotective novel  agents, but without such data for older drugs, leaving a critical gap in our  understanding of the relative effects of T2DM agents on cardiovascular risk.  METHODS AND ANALYSIS: The large-scale evidence generations across a network of  databases for T2DM (LEGEND-T2DM) initiative is a series of systematic,  large-scale, multinational, real-world comparative cardiovascular effectiveness  and safety studies of all four major second-line anti-hyperglycaemic agents,  including sodium-glucose co-transporter-2 inhibitor, glucagon-like peptide-1  receptor agonist, dipeptidyl peptidase-4 inhibitor and sulfonylureas. LEGEND-T2DM  will leverage the Observational Health Data Sciences and Informatics (OHDSI)  community that provides access to a global network of administrative claims and  electronic health record data sources, representing 190\,million patients in the  USA and about 50\,million internationally. LEGEND-T2DM will identify all adult,  patients with T2DM who newly initiate a traditionally second-line T2DM agent.  Using an active comparator, new-user cohort design, LEGEND-T2DM will execute all  pairwise class-versus-class and drug-versus-drug comparisons in each data source,  producing extensive study diagnostics that assess reliability and  generalisability through cohort balance and equipoise to examine the relative  risk of cardiovascular and safety outcomes. The primary cardiovascular outcomes  include a composite of major adverse cardiovascular events and a series of safety  outcomes. The study will pursue data-driven, large-scale propensity adjustment  for measured confounding, a large set of negative control outcome experiments to  address unmeasured and systematic bias. ETHICS AND DISSEMINATION: The study  ensures data safety through a federated analytic approach and follows research  best practices, including prespecification and full disclosure of results.  LEGEND-T2DM is dedicated to open science and transparency and will publicly share  all analytic code from reproducible cohort definitions through turn-key software,  enabling other research groups to leverage our methods, data and results to  verify and extend our findings.},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2022. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.},
  langid = {english},
  pmcid = {PMC9185490},
  pmid = {35680274},
  keywords = {*Diabetes Mellitus Type 2/chemically induced/drug therapy,*Dipeptidyl-Peptidase IV Inhibitors/therapeutic use,*Sodium-Glucose Transporter 2 Inhibitors/therapeutic use,Adult,Cardiology,DIABETES & ENDOCRINOLOGY,Health informatics,Humans,Hypoglycemic Agents/adverse effects,Reproducibility of Results,Sulfonylurea Compounds/therapeutic use}
}

@book{ohdsiBookOHDSIObservational2019,
  title = {The {{Book}} of {{OHDSI}}: {{Observational Health Data Sciences}} and {{Informatics}}},
  author = {{OHDSI}},
  year = {2019},
  publisher = {OHDSI},
  isbn = {978-1-0888-5519-5}
}

@article{rassenOnetomanyPropensityScore2012,
  title = {One-to-Many Propensity Score Matching in Cohort Studies},
  author = {Rassen, Jeremy A. and Shelat, Abhi A. and Myers, Jessica and Glynn, Robert J. and Rothman, Kenneth J. and Schneeweiss, Sebastian},
  year = {2012},
  month = may,
  journal = {Pharmacoepidemiology and Drug Safety},
  volume = {21},
  number = {S2},
  pages = {69--80},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1053-8569},
  doi = {10.1002/pds.3263},
  urldate = {2024-12-18},
  abstract = {ABSTRACT Background Among the large number of cohort studies that employ propensity score matching, most match patients 1:1. Increasing the matching ratio is thought to improve precision but may come with a trade-off with respect to bias. Objective To evaluate several methods of propensity score matching in cohort studies through simulation and empirical analyses. Methods We simulated cohorts of 20?000 patients with exposure prevalence of 10\%?50\%. We simulated five dichotomous and five continuous confounders. We estimated propensity scores and matched using digit-based greedy (?greedy?), pairwise nearest neighbor within a caliper (?nearest neighbor?), and a nearest neighbor approach that sought to balance the scores of the comparison patient above and below that of the treated patient (?balanced nearest neighbor?). We matched at both fixed and variable matching ratios and also evaluated sequential and parallel schemes for the order of formation of 1:n match groups. We then applied this same approach to two cohorts of patients drawn from administrative claims data. Results Increasing the match ratio beyond 1:1 generally resulted in somewhat higher bias. It also resulted in lower variance with variable ratio matching but higher variance with fixed. The parallel approach generally resulted in higher mean squared error but lower bias than the sequential approach. Variable ratio, parallel, balanced nearest neighbor matching generally yielded the lowest bias and mean squared error. Conclusions 1:n matching can be used to increase precision in cohort studies. We recommend a variable ratio, parallel, balanced 1:n, nearest neighbor approach that increases precision over 1:1 matching at a small cost in bias. Copyright ? 2012 John Wiley \& Sons, Ltd.},
  keywords = {comparative effectiveness research,confounding factors (epidemiology),epidemiologic methods,propensity scores}
}

@article{rosenbaumCentralRolePropensity1983,
  title = {The {{Central Role}} of the {{Propensity Score}} in {{Observational Studies}} for {{Causal Effects}}},
  author = {Rosenbaum, Paul R. and Rubin, Donald B.},
  year = {1983},
  journal = {Biometrika},
  volume = {70},
  number = {1},
  eprint = {2335942},
  eprinttype = {jstor},
  pages = {41--55},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {00063444, 14643510},
  doi = {10.2307/2335942},
  urldate = {2024-12-18},
  abstract = {[The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.]}
}

@article{ryanEvaluatingPerformanceRisk2013,
  title = {Evaluating {{Performance}} of {{Risk Identification Methods Through}} a {{Large-Scale Simulation}} of {{Observational Data}}},
  author = {Ryan, Patrick B. and Schuemie, Martijn J.},
  year = {2013},
  month = oct,
  journal = {Drug Safety},
  volume = {36},
  number = {1},
  pages = {171--180},
  issn = {1179-1942},
  doi = {10.1007/s40264-013-0110-2},
  abstract = {There has been only limited evaluation of statistical methods for identifying safety risks of drug exposure in observational healthcare data. Simulations can support empirical evaluation, but have not been shown to adequately model the real-world phenomena that challenge observational analyses.}
}

@article{schuemieImprovingReproducibilityUsing2018,
  title = {Improving Reproducibility by Using High-Throughput Observational Studies with Empirical Calibration.},
  author = {Schuemie, Martijn J. and Ryan, Patrick B. and Hripcsak, George and Madigan, David and Suchard, Marc A.},
  year = {2018},
  month = sep,
  journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
  volume = {376},
  number = {2128},
  address = {England},
  issn = {1471-2962 1364-503X},
  doi = {10.1098/rsta.2017.0356},
  abstract = {Concerns over reproducibility in science extend to research using existing healthcare data; many observational studies investigating the same topic produce  conflicting results, even when using the same data. To address this problem, we  propose a paradigm shift. The current paradigm centres on generating one estimate  at a time using a unique study design with unknown reliability and publishing (or  not) one estimate at a time. The new paradigm advocates for high-throughput  observational studies using consistent and standardized methods, allowing  evaluation, calibration and unbiased dissemination to generate a more reliable  and complete evidence base. We demonstrate this new paradigm by comparing all  depression treatments for a set of outcomes, producing 17\,718 hazard ratios, each  using methodology on par with current best practice. We furthermore include  control hypotheses to evaluate and calibrate our evidence generation process.  Results show good transitivity and consistency between databases, and agree with  four out of the five findings from clinical trials. The distribution of effect  size estimates reported in the literature reveals an absence of small or null  effects, with a sharp cut-off at p\,=\,0.05. No such phenomena were observed in our  results, suggesting more complete and more reliable evidence.This article is part  of a discussion meeting issue 'The growing ubiquity of algorithms in society:  implications, impacts and innovations'.},
  copyright = {{\copyright} 2018 The Authors.},
  langid = {english},
  pmcid = {PMC6107542},
  pmid = {30082302},
  keywords = {medicine,observational research,publication bias,reproducibility}
}

@article{schuemiePrinciplesLargescaleEvidence2020,
  title = {Principles of {{Large-scale Evidence Generation}} and {{Evaluation}} across a {{Network}} of {{Databases}} ({{LEGEND}}).},
  author = {Schuemie, Martijn J. and Ryan, Patrick B. and Pratt, Nicole and Chen, RuiJun and You, Seng Chan and Krumholz, Harlan M. and Madigan, David and Hripcsak, George and Suchard, Marc A.},
  year = {2020},
  month = aug,
  journal = {Journal of the American Medical Informatics Association : JAMIA},
  volume = {27},
  number = {8},
  pages = {1331--1337},
  address = {England},
  issn = {1527-974X 1067-5027},
  doi = {10.1093/jamia/ocaa103},
  abstract = {Evidence derived from existing health-care data, such as administrative claims and electronic health records, can fill evidence gaps in medicine. However, many  claim such data cannot be used to estimate causal treatment effects because of  the potential for observational study bias; for example, due to residual  confounding. Other concerns include P hacking and publication bias. In response,  the Observational Health Data Sciences and Informatics international  collaborative launched the Large-scale Evidence Generation and Evaluation across  a Network of Databases (LEGEND) research initiative. Its mission is to generate  evidence on the effects of medical interventions using observational health-care  databases while addressing the aforementioned concerns by following a recently  proposed paradigm. We define 10 principles of LEGEND that enshrine this new  paradigm, prescribing the generation and dissemination of evidence on many  research questions at once; for example, comparing all treatments for a disease  for many outcomes, thus preventing publication bias. These questions are answered  using a prespecified and systematic approach, avoiding P hacking. Best-practice  statistical methods address measured confounding, and control questions (research  questions where the answer is known) quantify potential residual bias. Finally,  the evidence is generated in a network of databases to assess consistency by  sharing open-source analytics code to enhance transparency and reproducibility,  but without sharing patient-level information. Here we detail the LEGEND  principles and provide a generic overview of a LEGEND study. Our companion paper  highlights an example study on the effects of hypertension treatments, and  evaluates the internal and external validity of the evidence we generate.},
  copyright = {{\copyright} The Author(s) 2020. Published by Oxford University Press on behalf of the American Medical Informatics Association.},
  langid = {english},
  pmcid = {PMC7481029},
  pmid = {32909033},
  keywords = {*Computer Communication Networks,*Databases Factual,*Meta-Analysis as Topic,Antihypertensive Agents/adverse effects/*therapeutic use,Confidence Intervals,Data Interpretation Statistical,empirical calibration,Humans,Hypertension/*drug therapy,Observation,observational studies,open science,Propensity Score,Randomized Controlled Trials as Topic,treatment effects,Treatment Outcome}
}

@article{tianEvaluatingLargescalePropensity2018,
  title = {Evaluating Large-Scale Propensity Score Performance through Real-World and Synthetic Data Experiments.},
  author = {Tian, Yuxi and Schuemie, Martijn J. and Suchard, Marc A.},
  year = {2018},
  month = dec,
  journal = {International journal of epidemiology},
  volume = {47},
  number = {6},
  pages = {2005--2014},
  address = {England},
  issn = {1464-3685 0300-5771},
  doi = {10.1093/ije/dyy120},
  abstract = {BACKGROUND: Propensity score adjustment is a popular approach for confounding control in observational studies. Reliable frameworks are needed to determine  relative propensity score performance in large-scale studies, and to establish  optimal propensity score model selection methods. METHODS: We detail a propensity  score evaluation framework that includes synthetic and real-world data  experiments. Our synthetic experimental design extends the 'plasmode' framework  and simulates survival data under known effect sizes, and our real-world  experiments use a set of negative control outcomes with presumed null effect  sizes. In reproductions of two published cohort studies, we compare two  propensity score estimation methods that contrast in their model selection  approach: L1-regularized regression that conducts a penalized likelihood  regression, and the 'high-dimensional propensity score' (hdPS) that employs a  univariate covariate screen. We evaluate methods on a range of outcome-dependent  and outcome-independent metrics. RESULTS: L1-regularization propensity score  methods achieve superior model fit, covariate balance and negative control bias  reduction compared with the hdPS. Simulation results are mixed and fluctuate with  simulation parameters, revealing a limitation of simulation under the  proportional hazards framework. Including regularization with the hdPS reduces  commonly reported non-convergence issues but has little effect on propensity  score performance. CONCLUSIONS: L1-regularization incorporates all covariates  simultaneously into the propensity score model and offers propensity score  performance superior to the hdPS marginal screen.},
  langid = {english},
  pmcid = {PMC6280944},
  pmid = {29939268},
  keywords = {*Observational Studies as Topic/methods/standards,*Propensity Score,*Research Design/standards/statistics & numerical data,Algorithms,Bias,Confounding Factors Epidemiologic,Humans,Logistic Models,Sample Size}
}

@article{grahamCardiovascularBleedingMortality2015,
  title = {Cardiovascular, {{Bleeding}}, and {{Mortality Risks}} in {{Elderly Medicare Patients Treated With Dabigatran}} or {{Warfarin}} for {{Nonvalvular Atrial Fibrillation}}},
  author = {Graham, David J. and Reichman, Marsha E. and Wernecke, Michael and Zhang, Rongmei and Southworth, Mary Ross and Levenson, Mark and Sheu, Ting-Chang and Mott, Katrina and Goulding, Margie R. and Houstoun, Monika and MaCurdy, Thomas E. and Worrall, Chris and Kelman, Jeffrey A.},
  year = {2015},
  month = jan,
  journal = {Circulation},
  volume = {131},
  number = {2},
  pages = {157--164},
  publisher = {American Heart Association},
  doi = {10.1161/CIRCULATIONAHA.114.012061},
  urldate = {2025-01-16}
}

@article{austinRelativeAbilityDifferent2009,
  title = {The Relative Ability of Different Propensity Score Methods to Balance Measured Covariates between Treated and Untreated Subjects in Observational Studies.},
  author = {Austin, Peter C.},
  year = {2009},
  journal = {Medical decision making : an international journal of the Society for Medical Decision Making},
  volume = {29},
  number = {6},
  pages = {661--677},
  address = {United States},
  issn = {1552-681X 0272-989X},
  doi = {10.1177/0272989X09341755},
  abstract = {The propensity score is a balancing score: conditional on the propensity score, treated and untreated subjects have the same distribution of observed baseline  characteristics. Four methods of using the propensity score have been described  in the literature: stratification on the propensity score, propensity score  matching, inverse probability of treatment weighting using the propensity score,  and covariate adjustment using the propensity score. However, the relative  ability of these methods to reduce systematic differences between treated and  untreated subjects has not been examined. The authors used an empirical case  study and Monte Carlo simulations to examine the relative ability of the 4  methods to balance baseline covariates between treated and untreated subjects.  They used standardized differences in the propensity score matched sample and in  the weighted sample. For stratification on the propensity score, within-quintile  standardized differences were computed comparing the distribution of baseline  covariates between treated and untreated subjects within the same quintile of the  propensity score. These quintile-specific standardized differences were then  averaged across the quintiles. For covariate adjustment, the authors used the  weighted conditional standardized absolute difference to compare balance between  treated and untreated subjects. In both the empirical case study and in the Monte  Carlo simulations, they found that matching on the propensity score and weighting  using the inverse probability of treatment eliminated a greater degree of the  systematic differences between treated and untreated subjects compared with the  other 2 methods. In the Monte Carlo simulations, propensity score matching tended  to have either comparable or marginally superior performance compared with  propensity-score weighting.},
  langid = {english},
  pmid = {19684288},
  keywords = {Adrenergic beta-Antagonists/*therapeutic use,Aged,Aged 80 and over,Angiotensin-Converting Enzyme Inhibitors/*therapeutic use,Female,Heart Failure/*drug therapy,Humans,Male,Monte Carlo Method,Probability}
}

@article{pirracchioEvaluationPropensityScore2012,
  title = {Evaluation of the {{Propensity}} Score Methods for Estimating Marginal Odds Ratios in Case of Small Sample Size},
  author = {Pirracchio, Romain and {Resche-Rigon}, Matthieu and Chevret, Sylvie},
  year = {2012},
  month = may,
  journal = {BMC Medical Research Methodology},
  volume = {12},
  number = {1},
  pages = {70},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-70},
  abstract = {Propensity score (PS) methods are increasingly used, even when sample sizes are small or treatments are seldom used. However, the relative performance of the two mainly recommended PS methods, namely PS-matching or inverse probability of treatment weighting (IPTW), have not been studied in the context of small sample sizes.}
}

@article{luncefordStratificationWeightingPropensity2004,
  title = {Stratification and Weighting via the Propensity Score in Estimation of Causal Treatment Effects: A Comparative Study},
  author = {Lunceford, Jared K. and Davidian, Marie},
  year = {2004},
  month = oct,
  journal = {Statistics in Medicine},
  volume = {23},
  number = {19},
  pages = {2937--2960},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {0277-6715},
  doi = {10.1002/sim.1903},
  urldate = {2025-01-16},
  abstract = {Abstract Estimation of treatment effects with causal interpretation from observational data is complicated because exposure to treatment may be confounded with subject characteristics. The propensity score, the probability of treatment exposure conditional on covariates, is the basis for two approaches to adjusting for confounding: methods based on stratification of observations by quantiles of estimated propensity scores and methods based on weighting observations by the inverse of estimated propensity scores. We review popular versions of these approaches and related methods offering improved precision, describe theoretical properties and highlight their implications for practice, and present extensive comparisons of performance that provide guidance for practical use. Copyright ? 2004 John Wiley \& Sons, Ltd.},
  keywords = {covariate balance,double robustness,inverse-probability-of-treatment-weighted-estimator,observational data}
}

@article{schuemieHowConfidentAre2020,
  title = {How Confident Are We about Observational Findings in Healthcare: {{A}} Benchmark Study},
  author = {Schuemie, Martijn J and Cepeda, M Soledad and Suchard, Marc A and Yang, Jianxiao and Tian, Yuxi and Schuler, Alejandro and Ryan, Patrick B and Madigan, David and Hripcsak, George},
  year = {2020},
  month = jan,
  journal = {Harv. Data Sci. Rev.},
  volume = {2},
  number = {1},
  publisher = {MIT Press - Journals},
  issn = {2688-8513},
  doi = {10.1162/99608f92.147cc28e},
  abstract = {Healthcare professionals increasingly rely on observational healthcare data, such as administrative claims and electronic health records, to estimate the causal effects of interventions. However, limited prior studies raise concerns about the real-world performance of the statistical and epidemiological methods that are used. We present the "OHDSI Methods Benchmark" that aims to evaluate the performance of effect estimation methods on real data. The benchmark comprises a gold standard, a set of metrics, and a set of open source software tools. The gold standard is a collection of real negative controls (drug-outcome pairs where no causal effect appears to exist) and synthetic positive controls (drug-outcome pairs that augment negative controls with simulated causal effects). We apply the benchmark using four large healthcare databases to evaluate methods commonly used in practice: the new-user cohort, self-controlled cohort, case-control, case-crossover, and self-controlled case series designs. The results confirm the concerns about these methods, showing that for most methods the operating characteristics deviate considerably from nominal levels. For example, in most contexts, only half of the 95\% confidence intervals we calculated contain the corresponding true effect size. We previously developed an "empirical calibration" procedure to restore these characteristics and we also evaluate this procedure. While no one method dominates, self-controlled methods such as the empirically calibrated self-controlled case series perform well across a wide range of scenarios.},
  keywords = {causal effect estimation,evaluation,methods,observational research}
}


