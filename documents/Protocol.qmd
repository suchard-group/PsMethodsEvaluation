---
title: "| RESEARCH PROTOCOL\n| \n| OHDSI propensity score matching evaluation study protocol\n  ? SPARTA: ? Survey of Propenstiy-score Adjustment Routes for Treatment-effect Analyses"
fontsize: 12pt
geometry: margin=1in
mainfont: Arial
bibliography: references.bib
csl: american-statistical-association.csl
number-sections: true
format: 
  html:
    toc: true
    toc-location: left
    html-math-method: katex
    grid:
      margin-width: 200px
    css: style.css
    # include-in-header:
    #   text: |
    #     \usepackage{longtable}
    #     \usepackage{unicode-math}
    #     \linespread{1.05}
    #     \usepackage{hyperref}
    #     \numberwithin{equation}{section}
    #     \usepackage{float}
    #     \floatplacement{figure}{H}
    #     \floatplacement{table}{H}
editor: visual
appendix-style: plain
toc: true

# TODO: Add back PDF rendering... colored table breaks with PDF
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(tidyverse)
# library(reticulate)
library(Matrix)
# library(MatrixModels)
library(knitr)
# library(patchwork)
# library(ggpubr)
library(wesanderson)
library(ggplot2)
library(kableExtra)
library(formattable)
options(knitr.kable.NA = '')
options(knitr.table.format = "html")

printCohortDefinitionFromNameAndJson <- function(name, json = NULL, obj = NULL,
                                                 withConcepts = TRUE,
                                                 withClosing = TRUE) {

  if (is.null(obj)) {
    obj <- CirceR::cohortExpressionFromJson(json)
  }

  writeLines(paste("####", name, "\n"))

  # Print main definition
  markdown <- CirceR::cohortPrintFriendly(obj)

  markdown <- gsub("criteria:\\r\\n ", "criteria:\\\r\\\n\\\r\\\n ", markdown)
  markdown <- gsub("old.\\r\\n\\r\\n", "old.\\\r\\\n", markdown)

  markdown <- gsub("The person exits the cohort", "\\\r\\\nThe person also exists the cohort", markdown)
  markdown <- gsub("following events:", "following events:\\\r\\\n", markdown)

  markdown <- sub("##### Inclusion Criteria", "##### Additional Inclusion Criteria\n", markdown)

  markdown <- unnumberAdditionalCriteria(markdown)
  markdown <- stringr::str_replace_all(
    markdown, "###### (\\d+).",
    function(matched_str) {
      digit <- stringr::str_extract(matched_str, stringr::regex("\\d+"))
      paste0("###### ", utils::as.roman(digit), ".")
    }
  )

  rows <- unlist(strsplit(markdown, "\\r\\n"))
  rows <- gsub("^   ", "", rows)
  markdown <- paste(rows, collapse = "\n")

  writeLines(markdown)

  # Print concept sets

  if (withConcepts) {
    lapply(obj$conceptSets, printConceptSet)
  }

  if (withClosing) {
    printCohortClose()
  }
}


# Uncomment below if you want code captions
# oldSource <- knitr::knit_hooks$get("source")
# knitr::knit_hooks$set(source = function(x, options) {
#   x <- oldSource(x, options)
#   x <- ifelse(!is.null(options$code.cap), paste0(x, "\\captionof{chunk}{", options$code.cap,"}"), x)
#   ifelse(!is.null(options$ref), paste0(x, "\\label{", options$ref,"}"), x)
# })
# Add `chunkcaption: TRUE` to YAML as well.
```


\newpage

## List of abbreviations

```{r}
abbrevs <- rbind(c("CDM", "Common data model"),
                  c("DPP4", "Dipeptidyl peptidase-4"),
                 c("GLP1", "Glucagon-like peptide-1"),
                 c("SGLT2", "Sodium-glucose co-transporter-2"),
                 c("SU", "Sulfanylurea"),
                 c("LEGEND", "Large-scale Evidence Generation and Evaluation across a Network of Databases"),
                 c("MACE", "Major adverse cardiovascular event"),
                 c("OHDSI", "Observational Health Data Science and Informatics"),
                 c("PS", "Propensity score"))

abbrevtbl <- kbl(abbrevs,
                 booktabs = T,
                 align = "ll",
                 linesep = "",
                 col.names = NULL) %>%
      kable_styling(latex_options = "striped",
                full_width = F) 
```

```{r}
abbrevtbl
```

## Responsible parties

### Investigators
```{r parties, echo=FALSE, message=FALSE}
parties <- readr::read_delim(col_names = TRUE, delim = ";", trim_ws = TRUE, file = "
  Investigator; Institution/Affiliation
  Kelly Li *; Department of Biostatistics, University of California, Los Angeles, Los Angeles, CA, USA
  Yong Chen; Department of Biostatistics, University of Pennsylvania, Philadelphia, PA, USA
  George Hripcsack; Department of Biomedical Informatics, Columbia University, New York, NY, USA
  Aki Nishimura; Department of Biostatistics, Johns Hopkins University, Baltimore, MD, USA
  Nicole Pratt; Department of Biostatistics, University of South Australia, Adelaide, SA, AU
  Patrick B. Ryan; Observational Health Data Analytics, Janssen Research and Development, Titusville, NJ, USA
  Martijn J. Schuemie; Observational Health Data Analytics, Janssen Research and Development, Titusville, NJ, USA
  Linying Zhang; Institute for Informatics, Data Science, & Biostatistics, Washington University, St. Louis, MO, USA
  Marc A. Suchard *; Department of Biostatistics, University of California, Los Angeles, Los Angeles, CA, USA
")

tab <- kable(parties, booktabs = TRUE, linesep = "") %>%
  column_spec(1, width = "10em") %>%
  column_spec(2, width = "35em") %>%
  footnote(general = "* Principal Investigator", general_title = "")

if (knitr::is_latex_output()) {
  tab %>% kable_styling(latex_options = c("striped", "hold_position"),
                        font_size = latex_table_font_size)
} else {
  tab %>% kable_styling(bootstrap_options = "striped")
}
```

### Disclosures
This study is undertaken within Observational Health Data Sciences and Informatics (OHDSI), an open collaboration.
**MJS** and **PBR** are employees of Janssen Research and Development and shareholders in John & Johnson.
**GH** receives grant funding from the US National Institutes of Health and the US Food & Drug Administration and contracts from Janssen Research and Development.
**HMK** receives grants from the US Food & Drug Administration, Medtronics and Janssen Research and Development, is co-founder of HugoHealth and chairs the Cardiac Scientific Advisory Board for UnitedHealth.
**MAS** receives grant funding from the US National Institutes of Health, the US Department of Veterans Affairs and the US Food & Drug Administration and contracts from Janssen Research and Development and IQVIA.
<!-- To be added... other collaborators -->

## Abstract

The new-user cohort method is utilized on observational data to estimate effects between groups. 
However, baseline differences between patient groups can introduce bias in treatment effect estimates. 
Propensity score (PS) models seek to address this issue to reduce bias in the overall outcome model. 
There is a variety of PS adjustment techniques, including matching, stratification, and weighting. 
Much research is needed to benchmark and study the bias-variance trade-off of these different techniques, particularly on large-scale PS models that can utilize up to $10^5 ~ 10^6$ covariates in the model. 
Much of the existing evidence is either based on simulations that may not fully capture the complexities of real-world data, or utilizing small-scale PS models with only 10-20 hand-picked covariates. 
This study aims to evaluate the performance of different PS adjustment strategies in a large-scale, real-world cohort of patients with type 2 diabetes mellitus (T2DM) initiating second-line treatment.

We specifically compare several PS adjustment methods, including 1-to-1 and 1-to-many matching, variable-ratio matching, stratification with 5, 10, and 20 groups, inverse probability of treatment weighting (IPTW), and overlap weighting.
In addition, we consider both large-scale propensity score (LSPS) models, utilizing OHDSI's standard sets of $10^5 ~ 10^6$ covariates and small-scale propensity score (SSPS) models, utilizing roughly 20-30 handpicked variables. 
To assess the bias-variance trade-offs of these techniques, we use a set of negative control outcomes, and a set of synthetic positive controls with predetermined effect sizes.
We aim to conduct a comprehensive evaluation of the trade-offs of utilizing different PS adjustment strategies.
The insights gained from this analysis on propensity score techniques are expected to guide decision-making in future studies utilizing observational healthcare data.

<!-- ## Amendments and Updates -->

<!-- ## Milestones -->

## Rationale and Background

### The cohort method and propensity scores
The new-user cohort method attempts to emulate randomized clinical trials for observational data [@hernanUsingBigData2016a]. 
Subjects who are observed to initiate a treatment of interest (the target) are compared to subjects initiating another treatment (the comparator) and are followed for a pre-specified amount of time following treatment initiation. 
One main difference between a randomized clinical trial and the cohort method is that there is no randomization of subjects between target and comparator, and hence patients receiving the target may systematically differ from patients receiving the comparator treatment [@rosenbaumCentralRolePropensity1983;@austinIntroductionPropensityScore2011].
Our solution of interest is to estimate a propensity score (PS), the baseline probability that a patient will receive one treatment over the other, across both patient groups. 
By matching, stratifying, or weighting patients based on their propensity scores, researchers aim to remove bias in the outcome risk estimate while achieving balance in the measured covariates between treatment groups.

### Variable selection
The PS is typically modeled using a binary model, such as logistic regression, applied to baseline patient characteristics.
The choice of characteristics going into the model can vary.
Some researchers opt to hand-pick a small set (between 10-50) of characteristics that may be related to the outcome.
Some recent examples of this are seen in a comparative safety study of anticoagulants [@grahamCardiovascularBleedingMortality2015], a comparison of diets in chronic kidney disease patients [@calabreseReviewPracticalExcursus2023], and a study of the effect of loneliness on unemployment [@morrishUnderstandingEffectLoneliness2022].
In practice, it may be very difficult to accurately classify a patient's baseline characteristics for whether or not they affect only the outcome, or exposure, or any combination of factors. 
In fact, many baseline covariates are likely to affect both the treatment assignment and the outcome.
@austinIntroductionPropensityScore2011 note that in many settings, researchers can safely include all measured baseline characteristics in the propensity score model.
OHDSI takes this larger approach, where the PS is modeled using a large-scale logistic regression model that utilizes between 10,000 and 100,000 baseline patient characteristics [@ohdsiBookOHDSIObservational2019;@tianEvaluatingLargescalePropensity2018;@zhangAdjustingIndirectlyMeasured2022].
These patient characteristics are generic, implying that they are not selected based on the treatment or outcomes of interest, and include demographics, as well as all diagnoses, drug exposures, measurement, and medical procedures.
This approach requires additional model fitting strategies due to its sheer dimension to prevent sparse data bias or even non-convergent estimates, and are typically fit using large-scale regularized regression via OHDSI's collection of open-source data analytics tools [@suchardMassiveParallelizationSerial2013;@schuemieHealthAnalyticsDataEvidence2024].
In this study, we plan to compare both small-scale propensity score models (SSPS) with a small set of hand-picked covariates and large-scale propensity score models (LSPS) utilizing a set of order $10^5$ generic patient characteristics.

### Adjustment types
Another factor to consider once the covariates for the model are chosen is in the types of adjustment conducted after the model is fit. 
Researchers can choose to match, stratify, or weigh the patients based on their propensity scores.
There is a sizable amount of research that compares these different PS adjustment methods, but we note that a large amount of its conclusions are drawn from simulations which have an unclear relationship with the real world, where residual systematic error persists. 
In addition, the majority of them utilize simulated data on a smaller scale model with only a few ($\leq 100$) covariates.
We would like to compare those existing results with a large-scale model utilizing real-world data to evaluate the trade-offs of each adjustment method.

There are a few different ways to match patients on their PS; the options are fixed ratio, where each subject receiving the treatment of interest are matched to a fixed number of controls (one-to-one or one-to-many), or variable ratio (VR), where each subject receives a variable number of matches.
For matching, @rosenbaumCentralRolePropensity1983;@cochranControllingBiasObservational1973 recommend a "caliper" of 0.2 standard deviations on the logit scale, where patients are able to be matched together if their scores fall within this given amount. 
It is hypothesized that matching a target subject to more than one comparator will reduce the variance of the eventual treatment effect estimate, because more patient data is able to be utilized in the overall effect estimation.
On the other hand, this might yield a higher bias, since each subsequent match for the patient will be of lower quality than the first initial one.
@rassenOnetomanyPropensityScore2012 recommend VR matching among all of the matching options.
They found that in a simulation of size 50,000 patients and 8 covariates, VR matching had the highest precision with only a small bias trade-off cost when compared to fixed-ratio matching for one-to-one and one-to-n with varying n.
This is because that variable ratio matching retains treated subjects who do not meet the criteria of fixed number matches (i.e. a target patient only matches to one comparator, but fixed-ratio $1:3$ matching requires that same target patient to have three matches in order to be included in the study). 

In addition to matching, researchers can choose to stratify or weigh patients based on their PS. 
@rosenbaumReducingBiasObservational1984 recommend stratifying patients into 5 quintiles with the same caliper as matching, 0.2 standard deviations on the logit scale.
Weighting is conducted using inverse propensity score treatment weighting (IPTW), where patients are assigned weights based on the inverse of their propensity score.
One common issue with IPTW is that it does not handle extreme weights well: patients who are very unlikely to be assigned the target treatment with low PS will have very heavy inverse weights.
There are two ways to handle this; the first is by trimming the upper and lower extremes, typically 5%, of the PS estimates.
The second is via overlap weights, which down-weights the tails of the distribution of weights such that the population in the middle, who have a lot of characteristic overlap, will be emphasized [@thomasOverlapWeightingPropensity2020;@liAddressingExtremePropensity2019].
@austinRelativeAbilityDifferent2009 compares IPTW with trimming to stratification and one-to-one matching on a small-scale (1000 patients and 6 covariates) simulation, along with a real-world empirical study on two cardiovascular drugs with a small hand-picked covariate set and finds that IPTW and one-to-one matching performed the best at balancing treatment groups.
In a comparison of just stratification and IPTW, @luncefordStratificationWeightingPropensity2004 find that IPTW outperforms stratification in a small-scale simulation (1000-5000 patients with 8 covariates) on both bias and precision.
Finally, @pirracchioEvaluationPropensityScore2012 report that IPTW outperforms matching on both bias and precision when sample sizes are very small on simulated data from 1000 patients all the way down to 40 patients with 4 covariates.
Again, we note that these existing benchmarks 

When comparing varying PS adjustment methods in LSPS within the OHDSI framework, @schuemieHowConfidentAre2020 finds that, in comparison to matching and stratification, IPTW drastically underperforms across almost all metrics of success on an analysis utilizing real-world data with order $10^5$ covariates.
In the LSPS study, arguments can be made for one-to-one and VR matching as the optimal adjustment methods based on their precision and bias measurements [@schuemieHowConfidentAre2020].
We note this as a stark contrast to existing benchmarks [@austinRelativeAbilityDifferent2009;@luncefordStratificationWeightingPropensity2004;@pirracchioEvaluationPropensityScore2012], which seem to promote IPTW as the recommended method.
We hypothesize that this is because they are all mostly based on simulated data and models that utilize very small covariate sets, while @schuemieHowConfidentAre2020 applied the methods to real-world data with a very large covariate set in comparison.
In this study, we hope to conduct a comprehensive evaluation of many of the choices that go into a PS model.
We will investigate the impact of both small-scale models with hand-picked covariates and large-scale models with many generic characteristics, along with the trade-offs of the different adjustment methods, including fixed-ratio and variable-ratio matching, IPTW with trimming and overlap weights, and stratification into varying strata counts.
We hope that the insights gained from this analysis on propensity score techniques will help guide decision-making in future studies utilizing observational healthcare data.

<!-- [MAS: maybe also some discussion that different PS schemes leads to slightly different effect estimates, i.e. ATT, ATE, ATO, etc.]{.aside} -->

## Study Objectives

To inform decision-making in observational health research for propensity score adjustments, we have launched the Propensity-score Methods Evaluation (PME) initiative to execute a series of comprehensive observational studies to compare varying choices in the development of the PS model, including variable selection and the adjustment method (matching, stratifying, weighting). 
Specifically, these studies aim

  1. To determine, through systematic evaluation, the performance of a small-scale propensity score (SSPS) model that utilizes a small set of $<100$ hand-picked covariates, against a large-scale propensity score (LSPS) model that utilizes a large set (dimension $10^5$) of generic patient covariates.
  2. To determine, through systematic evaluation, the comparative performance of different PS adjustment methods, including varying matching techniques, stratification into different strata counts, and IPTW with different ways to account for extreme weights.
  3. To explain why OHDSI's existing benchmarks consider IPTW an under-performing method compared to the other options, while other researchers recommend and utilize IPTW highly frequently.

## Methods

### Study Design 

#### Propensity score model specifications {#sec-analyseslist}

Our evaluation focuses on the differences in PS adjustment schemes applied to an outcome model and its included patients to estimate a relative effect.
Each variant assumes a popular Cox proportional hazards approach to model time-to-event until first outcome or censoring after treatment initiation with a single treatment-indicator covariate.
Variants further condition (or not) the Cox model to have independent baseline hazard functions across matched-set and strata.

We first consider the differences in large-scale propensity score (LSPS) and small-scale propensity score (SSPS) models.
Previously, SSPS models were fit using a relatively small subset of $<100$ hand-picked characteristics based on expert clinical opinions; the goal was to select confounders that were related to the outcome.
Our SSPS models include roughly $50$ hand-picked covariates, selected by using existing research on our treatments of interest and expert opinions for potential confounding characteristics.
@sec-ssps lists the chosen covariates of interest for the SSPS models.
LSPS models, on the other hand, use a large set of generic patient characteristics that may or may not be related to the treatment or outcome; they are simply a generic subset of all possible measured covariates for a patient.
A model typically involves between 10,000 to 100,000 characteristics (in this case, closer to order 10,000) and include demographics, as well as all diagnoses, drug exposures, measurement, and medical procedures observed prior to treatment initiation, and exclude the target and comparator treatment.
@sec-lsps lists the categories for the generic characteristics included in the LSPS models.

<!-- TODO?: frequency / characterization for the small-scale covariates, like in Graham's paper : at least a description of _why_ these were picked -->

When matching and stratifying by PS, we use a recommended caliper of 0.2 standard deviations on the standardized logit scale [@cochranControllingBiasObservational1973].
Our matching variants include both variable and fixed one-to-one or one-to-many matching ratios.
For stratification, the recommended number of strata is 5 [@rosenbaumReducingBiasObservational1984], but we also express interest in the effects of 10 and 20 strata.
We also investigate 2 different ways to counter extreme weights for IPTW: the first trims the weights at the 2.5 and 97.5 percentile, and the second uses overlap weights that emphasize patients with more characteristic overlap.

@tbl-1 lists each variant to be evaluated. 

```{r}
baseline1 <- c("Unadjusted")
matching1 <- c("Matching (1:1)", "Matching (1:1)", "Matching (1:5)", "Matching (1:10)", "Matching (variable-ratio)")
strat1 <- c("Stratification (5 groups)", "Stratification (10 groups)", "Stratification (20 groups)")
weight1 <- c("IPTW", "Overlap weighing")

lsps <- tibble(method = c(matching1, strat1, weight1),
               stratified = "Y")

lsps$stratified[c(1, 9:10)] <- "N"
lsps$model <- "LSPS"
lsps$id <- c(10:19)

# lsps$model[1] <- "None"

# matching2 <- c("Matching (1:1)", "Matching (1:5)", "Matching (variable-ratio)")
# strat2 <- c("Stratification (5 groups)")
# weight2 <- c("IPTW")
# 
# ssps <- tibble(method = c(matching2, strat2, weight2),
#                stratified = "Y")
# ssps$stratified <- "N"
# ssps$stratified[3] <- "Y"
ssps <- lsps
ssps$model <- "SSPS"
ssps$id <- c(20:29)

base <- tibble(id = 1, method = "Unadjusted",
               stratified = "N",
               model = "None")

data1 <- rbind(base, lsps, ssps)

table1 <- kbl(data1,
              booktabs = T,
              col.names = c("ID",
                            "PS adjustment variant",
                            "Conditional outcome model",
                            "PS model type"),
              align = "rlcc",
              linesep = "") %>% 
    kable_styling(latex_options = "striped",
                full_width = F) 

```

```{r}
#| label: tbl-1
#| tbl-cap: "New-user cohort method analysis variants of propensity score (PS) adjustment of outcome model, its conditioning and patient usage. DEFINE LSPS, SSPS, etc. here"
table1
```

#### CohortMethod study design

Here we seek to evaluate the bias-variance trade-offs of different PS adjustment techniques in a large-scale real world study across multiple data sources, in this case comparing initiating different drug classes for 2nd-line type-2 diabetes (T2DM) treatment.
For each of the analyses specified in @sec-analyseslist, we employ an active comparator, new-user cohort design by selecting one target/comparator (exposure) pair, and all of the selected negative controls and synthesized positive controls described in @sec-tcos.

### Target, comparator, and outcome specifications {#sec-tcos}

#### Exposure comparators

We utilize four exposure cohorts for new-users of any drug ingredient wihin the four traditionally second-line drug classes for type-2-diabetes mellitus (T2DM) treatment: DPP4, GLP1, SGLT2, and SU. <!-- The index date for each patient is their first observed exposure to any drug ingredient for the four second-line drug classes.  --> 
@sec-cohortdefs reports the complete OHDSI `ATLAS` cohort descriptions for new-users of each drug class. 
The description includes complete specification of cohort entry events, inclusion criteria, cohort exit events, and all associated OMOP-CDM concept codes used in the definition of the cohort. 
For each data source presented in @tbl-datasources, we then execute pairwise class comparisons, where in each pair, each class has one opportunity to serve as the target and the other as the comparator. This results in $C \binom{4}{2} * 2 = 12$ combinations to execute.

```{r}
#| label: tbl-cohortcounts
#| tbl-cap: "Cohort counts for exposure comparators across datasources"

counts <- readRDS("./figures/cohortCounts.rds")
kbl(counts,
              booktabs = T,
              col.names = c("Cohort name",
                            "Cohort ID",
                            "CCAE",
                            "MDCD",
                            "MDCR",
                            "OptumEHR"),
              align = "llcccc",
              linesep = "") %>% 
    kable_styling(latex_options = "striped",
                full_width = F) 

```

@tbl-cohortcounts displays the cohort counts for each of the second-line drug class comparators.

#### Outcomes
To measure the bias trade-off in the presence of residual systematic error, we utilize a large set of negative control outcome experiments.
Negative control outcomes assume true the null hypothesis of no effect, where we do not believe that exposure to the target or the comparator will cause the outcome. 
In this study, we utilize the same negative controls as those in the LEGEND-T2DM study, which validated approximately 100 negative controls that were generated using a data-rich algorithm drawing upon literature, product labels, and spontaneous reports. 
[@kheraLargescaleEvidenceGeneration2022]. 
<!-- TODO: [MAS: Patrick added more negative controls into `Glp1Dili` that JnJ calls their "universal controls"; we should use all of these, and describe how they were generated.]{.aside} -->
The chosen negative control concepts are reported in @sec-ncs. 
For each negative control, we generate synthetic positive controls with target effect sizes of \[1.5, 2, 4\] by adding simulated outcomes to the existing negative control until the desired effect is reached [@colomaReferenceStandardEvaluation2013]. 
Utilizing negative and positive controls allow us to most importantly evaluate our model for bias, and also its discrimination ability against the null (0 effect), where negative controls are expected to accept the null, and positive controls are expected to reject.

<!-- The three main outcomes of interest are defined using manually crafted rule-based definitions using a combination of diagnosis concept codes in the OMOP-CDM: -->

<!-- -   3-point major adverse cardiovascular events (MACE), including acute myocardial infarction, stroke, and sudden cardiac death, and -->

<!-- -   4-point MACE that additionally includes heart failure hospitalization. -->

<!-- -   Acute renal failure -->

<!-- These outcome definitions have been previously implemented and validated in the `LEGEND-T2DM` study \[\@?\]. The formal `ATLAS` definitions of the outcomes are found below. -->

<!-- Another choice for adjusting for confounding is to include additional covariates in the outcome model. Typically, the outcome model with the propensity score is conditioned only on the propensity score and the treatment initiated. When outcomes are more rare, there may be a lack of data to fit an elaborate model. We can make the choice to add the same variables used in the PS model into the outcome model to adjust for the same variables two times in total, but with different methods. -->

### Data sources

We will evaluate all PS variants for all comparisons across a series of administrative claims and electronic health records (EHR) data through the OHDSI Evidence Network. 
The models are conveniently run across differing database types as a result of the community's shared Observational Medical Outcomes Partnership (OMOP) common data model (CDM) and the OHDSI Methods Library.

@tbl-datasources lists the 5 data sources that will be utilized in this evaluation, with a brief description and population size, along with information on the data capturing process for its patients and its start date.

```{r}
#| label: tbl-datasources
#| tbl-cap: "Committed data sources and their covered populations"

data_sources <- readr::read_delim(col_names = TRUE, delim = ";", trim_ws = TRUE, file = "
  Data source ; Population ; Patients ; History ; Data capture process and short description
  IBM MarketScan Commercial Claims and Encounters (CCAE) ; Commercially insured, < 65 years ; 142M ; 2000 -- ; Adjudicated health insurance claims (e.g. inpatient, outpatient, and outpatient pharmacy)  from large employers and health plans who provide private healthcare coverage to employees, their spouses and dependents.
  IBM MarketScan Medicare Supplemental Database (MDCR)  ; Commercially insured, 65+ years ; 10M ; 2000 -- ; Adjudicated health insurance claims of retirees with primary or Medicare supplemental coverage through privately insured fee-for-service, point-of-service or capitated health plans.
  IBM MarketScan Multi-State Medicaid Database (MDCD) ; Medicaid enrollees, racially diverse ; 26M ; 2006 -- ; Adjudicated health insurance claims for Medicaid enrollees from multiple states and includes hospital discharge diagnoses, outpatient diagnoses and procedures, and outpatient pharmacy claims.
  Optum Electronic Health Records (OptumEHR) ; US, general ; 93M ; 2006 -- ; Clinical information, prescriptions, lab results, vital signs, body measurements, diagnoses and procedures derived from clinical notes using natural language processing.
",
show_col_types = FALSE)
tab <- kable(data_sources, booktabs = TRUE, linesep = "") %>%
  kable_styling(bootstrap_options = "striped", latex_options = "striped") %>%
  pack_rows("Administrative claims", 1, 3, latex_align = "c", indent = FALSE) %>%
  pack_rows("Electronic health records (EHRs)", 4, 5, latex_align = "c", indent = FALSE)

if (knitr::is_latex_output()) {
  tab %>%
    column_spec(1, width = "6em") %>%
    column_spec(2, width = "6em") %>%
    column_spec(5, width = "20em")
} else {
  tab
}

```

### Metrics
Each propensity score adjustment method will be applied to one analysis estimating the treatment effects on the negative and positive controls.
We assume that each analysis will produce an effect size estimate for each control, uncertainty of the estimate expressed as a 95% confidence interval, and a two-sided p-value for the null hypothesis of no effect.
Prior work shows that the p-value and confidence intervals can be empirically calibrated to restore their operating characteristics to near-nominal values using the observed distribution of estimates for the negative and positive controls [@schuemieEmpiricalConfidenceInterval2018;@schuemieInterpretingObservationalStudies2014].

We will apply empirical calibration before computing and reporting the performance metrics adopted from previous research [@schuemieHowConfidentAre2020]:

  - Coverage: How often is the true effect size within the 95% confidence interval for each control?
  - Non-estimable: How many controls was the method unable to produce an estimate? This can be for many reasons, e.g. Not enough subjects left after PS adjustment.
  - Mean precision: computed as $\frac{1}{SE^2}$, where higher precision implies lower variance and narrower confidence intervals.
  - Mean squared error (MSE): mean squared difference between the estimated effect size and the true effect size
  - Type I error: For negative controls, how often was the null rejected ($\alpha = .05$)? This is equivalent to the false positive rate and 1 - specificity.
  - Type II error: For positive controls, how often was the null not rejected? ($\alpha = .05$)? This is equivalent to the false negative rate and 1 - sensitivity.
  - Sensitivity and specificity according to the previous definitions for Type I / II error.
  - AUC: How well can we discriminate between positive and negative controls?

The metrics will be computed overall, and also stratified by the true effect size, especially for the positive controls.
We will evaluate each adjustment method primarily for its bias-variance trade-off by examining the MSE and precision of the treatment effect estimates, before we take into consideration the other performance metrics.

<!-- ## Sample Size and Study Power -->

<!-- <!-- from MethodEvaluation: Within each database, the minimum detectable relative risk (MDRR) will be computed for each control as a typical proxy for power.  --> -->

<!-- We inspect the distributions of the PS estimates to evaluate population generalizability and cohort balance before and after adjustment.  -->
<!-- Finally, Kaplan-Meier plots are utilized to examine hazard ratio proportionality assumptions. [MAS: these two sentences have nothing to do with sample size or power]{.aside} -->

## Strengths and Limitations

### Strengths

We aim to conduct an empirical evaluation of a wide combination of propensity score adjustment methods to population-level estimation across databases containing large-scale administrative claims and EHR data. 
The existence of a large set of real-world negative controls and synthetic positive controls allow for quantification of unmeasured and systemic bias inherent in observational studies.

### Limitations

While the study uses a wide range of US health data, some operating characteristics will depend on the choice of database, and hence generalization may be difficult for databases outside of this study. 
In addition, the use of real-world negative control outcomes does not imply that the true confounding structure between treatments and outcomes is known. 
We utilize a large set of negative controls that represent a wide range of confounding structures, such that one method leading to reduced bias and higher precision for many structures implies those positive effects on the eventual effect estimates. 
On the other hand, synthetic positive controls only reflect measured bias, due to the use of injected outcomes based on negative controls. 
Finally, the use of real-world data contains certain issues, including limited and variable observed follow-up times, and missing visit/care episodes for patients. 
We believe such bias from using a real-world data source will likely be towards the null.

## Protection of Human Subjects

This study uses human data collected during routine healthcare provision, and all data is de-identified within the data source. 
All data partners executing the analyses within their data sources will have received institutional review board (IRB) approval or waiver for participation in accordance to their institutional governance prior to execution.
PME executes across a federated and distributed data network, where participating data partners only receive analysis code and return aggregate summary statistics, with no sharing of patient-level data between organizations.

<!-- todo: table 10.1, per EUMAEUS / BETTER protocol -->


## Sample results table

```{r}
#| label: tbl-metricsPreCali
#| tbl-cap: "Performance metrics before empirical calibration across all positive and negative controls"
metrics <- readRDS("./figures/metricsPreCali.rds")

metrics %>%
  mutate(
    auc = color_bar("lightpink")(auc),
    coverage = color_bar("lightpink")(coverage),
    meanP = color_bar("lightpink")(meanP),
    mse = color_bar("lightblue")(mse),
    type1 =color_bar("lightblue")(type1),
    type2 = color_bar("lightblue")(type2),
    nonEstimable = color_bar("lightgreen")(nonEstimable)
  ) %>%
  select(analysisDescription, auc, coverage, meanP,
         mse, type1, type2, nonEstimable) %>%
  kable(escape = F,
        booktabs = T,
        col.names = c("Description", "AUC", "Coverage", "Mean Precision",
                      "MSE", "Type-I Error", "Type-II Error", "Non-estimable (%)"))
```

```{r}
#| label: tbl-metricsPostCali
#| tbl-cap: "Performance metrics after empirical calibration for SGTL2is vs GLP1s across all positive and negative controls"
metricsCali <- readRDS("./figures/metricsCali.rds")
metricsCali %>%
  mutate(
    auc = color_bar("lightpink")(auc),
    coverage = color_bar("lightpink")(coverage),
    meanP = color_bar("lightpink")(meanP),
    mse = color_bar("lightblue")(mse),
    type1 =color_bar("lightblue")(type1),
    type2 = color_bar("lightblue")(type2),
    nonEstimable = color_bar("lightgreen")(nonEstimable)
  ) %>%
  select(analysisDescription, auc, coverage, meanP,
         mse, type1, type2, nonEstimable) %>%
  kable(escape = F,
        booktabs = T,
        col.names = c("Description", "AUC", "Coverage", "Mean Precision",
                      "MSE", "Type-I Error", "Type-II Error", "Non-estimable (%)"))
```

## Plans for Disseminating and Communicating Study Results

Open science aims to make scientific research, including its data processing, software, and dissemination, accessible to all levels of an inquiring society [@woelfleOpenScienceResearch2011].
Open science delivers reproducible, transparent, and reliable evidence.
All aspects of PME (except private patient data) will be open and we will actively encourage other interested researchers, clinicians, and patients to participate.
This differs fundamentally from traditional studies that rarely open their analytic tools or share all result artifacts, informing the community about hard-to-verify conclusions at completion.

### Transparent and re-usable research tools
We plan to publicly register this protocol and announce its availability for feedback from stakeholders, the OHDSI community, and within clinical professional societies.
This protocol will link to open source code for all the steps utilized to generate diagnostic measurements, effect estimates, figures, and tables. 
Such transparency is possible because we construct our studies on top of the OHDSI toolstack of open source software tools that are community developed and rigorously tested [@schuemieHowConfidentAre2020].
We will publicly host PME source code at *github.com/pme*, allowing public contribution and review, and free re-use for anybody's future research.

### Scientifc meetings and publications
We will deliver multiple presentations at scientific venues and will also prepare publications for clinical, informatics, and statistical journals.

### General public
We believe in sharing our findings that will guide observational health data research with the general public.
With dedicated support from the OHDSI communications specialist, we will deliver regular press releases at key project stages, distributed via the extensive media networks of UCLA and Columbia.

## Appendix

### Cohort definitions {#sec-cohortdefs}

The following is an example class cohort definition for DPP4-inhibitors. 
Cohort definitions for other classes are very similar.

```{r,  echo=FALSE, results="asis", warning=FALSE, message=FALSE}
source("./PrettyOutput.R")
printCohortDefinitionFromNameAndJson(name = "New-users of DDP4Is",
                                     json = SqlRender::readSql("../inst/cohorts/ID101100000.json"),
                                     withConcepts = TRUE)
```

### Small-scale propensity model {#sec-ssps}

All covariates for condition or drug use are defined as those from 365 days prior to index date.

```{r}
ssCovs <- readRDS("./figures/sspsRef.rds") %>%
  arrange(covariateId)

ssCovs$covariateName <- gsub(".*index: ", "", ssCovs$covariateName)

kbl(ssCovs,
    booktabs = T,
    col.names = c("Covariate ID",
                  "Covariate name",
                  "Analysis ID")) %>%
  kable_styling(latex_options = "striped",
                full_width = F)
```

### Large-scale propensity model {#sec-lsps}
The following features are used in the LSPS models:

Demographic data:
  - Gender
  - Age group (5 year bins)
  - Race
  - Ethnicity
  - Index year
  - Index month

Occuring between -365 days and 0 days relative to index:
  - Conditions
  - Drug groups
  - Procedure
  - Device exposure
  - Laboratory measurements
  - Observations
  
Occuring between -30 days and 0 days relative to index:
  - Conditions
  - Drug groups
  - Procedure
  - Device exposure
  - Laboratory measurements
  - Observations
  
Other indices:
  - Charlson comorbidity index 
  - Diabetes complications severity index
  - Congestive heart failure, hypertension, age > 75 years, diabetes, previous stroke (CHADS2) score
  - Congestive heart failure, hypertension, age > 75 years, diabetes, previous stroke, vascular disease (CHADS2-VASc) score

### Negative control concepts {#sec-ncs}

```{r}
ncs <- read.csv("../inst/settings/NegativeControls.csv") %>%
  select(outcomeId, outcomeName)

kbl(ncs,
    booktabs = T,
    col.names = c("Outcome ID",
                  "Name")) %>%
  kable_styling(latex_options = "striped",
                full_width = F)
```
