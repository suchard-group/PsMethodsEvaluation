---
title: "| RESEARCH PROTOCOL\n| \n| OHDSI propensity score matching evaluation study protocol\n  ? SPARTA: ? Survey of Propenstiy-score Adjustment Routes for Treatment-effect Analyses"
fontsize: 12pt
geometry: margin=1in
mainfont: Arial
bibliography: references.bib
csl: american-statistical-association.csl
number-sections: true
format: 
  html:
    toc: true
    toc-location: left
    html-math-method: katex
    grid:
      margin-width: 200px
    css: style.css
    # include-in-header:
    #   text: |
    #     \usepackage{longtable}
    #     \usepackage{unicode-math}
    #     \linespread{1.05}
    #     \usepackage{hyperref}
    #     \numberwithin{equation}{section}
    #     \usepackage{float}
    #     \floatplacement{figure}{H}
    #     \floatplacement{table}{H}
editor: visual
appendix-style: plain
toc: true

# TODO: Add back PDF rendering... colored table breaks with PDF
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(tidyverse)
# library(reticulate)
library(Matrix)
# library(MatrixModels)
library(knitr)
# library(patchwork)
# library(ggpubr)
library(wesanderson)
library(ggplot2)
library(kableExtra)
library(formattable)
options(knitr.kable.NA = '')
options(knitr.table.format = "html")

printCohortDefinitionFromNameAndJson <- function(name, json = NULL, obj = NULL,
                                                 withConcepts = TRUE,
                                                 withClosing = TRUE) {

  if (is.null(obj)) {
    obj <- CirceR::cohortExpressionFromJson(json)
  }

  writeLines(paste("####", name, "\n"))

  # Print main definition
  markdown <- CirceR::cohortPrintFriendly(obj)

  markdown <- gsub("criteria:\\r\\n ", "criteria:\\\r\\\n\\\r\\\n ", markdown)
  markdown <- gsub("old.\\r\\n\\r\\n", "old.\\\r\\\n", markdown)

  markdown <- gsub("The person exits the cohort", "\\\r\\\nThe person also exists the cohort", markdown)
  markdown <- gsub("following events:", "following events:\\\r\\\n", markdown)

  markdown <- sub("##### Inclusion Criteria", "##### Additional Inclusion Criteria\n", markdown)

  markdown <- unnumberAdditionalCriteria(markdown)
  markdown <- stringr::str_replace_all(
    markdown, "###### (\\d+).",
    function(matched_str) {
      digit <- stringr::str_extract(matched_str, stringr::regex("\\d+"))
      paste0("###### ", utils::as.roman(digit), ".")
    }
  )

  rows <- unlist(strsplit(markdown, "\\r\\n"))
  rows <- gsub("^   ", "", rows)
  markdown <- paste(rows, collapse = "\n")

  writeLines(markdown)

  # Print concept sets

  if (withConcepts) {
    lapply(obj$conceptSets, printConceptSet)
  }

  if (withClosing) {
    printCohortClose()
  }
}


# Uncomment below if you want code captions
# oldSource <- knitr::knit_hooks$get("source")
# knitr::knit_hooks$set(source = function(x, options) {
#   x <- oldSource(x, options)
#   x <- ifelse(!is.null(options$code.cap), paste0(x, "\\captionof{chunk}{", options$code.cap,"}"), x)
#   ifelse(!is.null(options$ref), paste0(x, "\\label{", options$ref,"}"), x)
# })
# Add `chunkcaption: TRUE` to YAML as well.
```


\newpage

## List of abbreviations

```{r}
abbrevs <- rbind(c("CDM", "Common data model"),
                  c("DPP4", "Dipeptidyl peptidase-4"),
                 c("GLP1", "Glucagon-like peptide-1"),
                 c("SGLT2", "Sodium-glucose co-transporter-2"),
                 c("SU", "Sulfanylurea"),
                 c("LEGEND", "Large-scale Evidence Generation and Evaluation across a Network of Databases"),
                 c("MACE", "Major adverse cardiovascular event"),
                 c("OHDSI", "Observational Health Data Science and Informatics"),
                 c("PS", "Propensity score"))

abbrevtbl <- kbl(abbrevs,
                 booktabs = T,
                 align = "ll",
                 linesep = "",
                 col.names = NULL) %>%
      kable_styling(latex_options = "striped",
                full_width = F) 
```

```{r}
abbrevtbl
```

## Responsible parties

### Investigators
```{r parties, echo=FALSE, message=FALSE}
parties <- readr::read_delim(col_names = TRUE, delim = ";", trim_ws = TRUE, file = "
  Investigator; Institution/Affiliation
  Kelly Li *; Department of Biostatistics, University of California, Los Angeles, Los Angeles, CA, USA
  Yong Chen; Department of Biostatistics, University of Pennsylvania, Philadelphia, PA, USA
  George Hripcsack; Department of Biomedical Informatics, Columbia University, New York, NY, USA
  Aki Nishimura; Department of Biostatistics, Johns Hopkins University, Baltimore, MD, USA
  Nicole Pratt; Department of Biostatistics, University of South Australia, Adelaide, SA, AU
  Patrick B. Ryan; Observational Health Data Analytics, Janssen Research and Development, Titusville, NJ, USA
  Martijn J. Schuemie; Observational Health Data Analytics, Janssen Research and Development, Titusville, NJ, USA
  Linying Zhang; Institute for Informatics, Data Science, & Biostatistics, Washington University, St. Louis, MO, USA
  Marc A. Suchard *; Department of Biostatistics, University of California, Los Angeles, Los Angeles, CA, USA
")

tab <- kable(parties, booktabs = TRUE, linesep = "") %>%
  column_spec(1, width = "10em") %>%
  column_spec(2, width = "35em") %>%
  footnote(general = "* Principal Investigator", general_title = "")

if (knitr::is_latex_output()) {
  tab %>% kable_styling(latex_options = c("striped", "hold_position"),
                        font_size = latex_table_font_size)
} else {
  tab %>% kable_styling(bootstrap_options = "striped")
}
```

### Disclosures
This study is undertaken within Observational Health Data Sciences and Informatics (OHDSI), an open collaboration.
**MJS** and **PBR** are employees of Janssen Research and Development and shareholders in John & Johnson.
**GH** receives grant funding from the US National Institutes of Health and the US Food & Drug Administration and contracts from Janssen Research and Development.
**HMK** receives grants from the US Food & Drug Administration, Medtronics and Janssen Research and Development, is co-founder of HugoHealth and chairs the Cardiac Scientific Advisory Board for UnitedHealth.
**MAS** receives grant funding from the US National Institutes of Health, the US Department of Veterans Affairs and the US Food & Drug Administration and contracts from Janssen Research and Development and IQVIA.
<!-- To be added... other collaborators -->

## Abstract

The new-user cohort method is utilized on observational data to estimate effects between groups. 
However, baseline differences between patient groups can introduce bias in treatment effect estimates. 
Propensity score (PS) models seek to address this issue to reduce bias in the overall outcome model. 
There is a variety of PS adjustment techniques, including matching, stratification, and weighting. 
Much research is needed to benchmark and study the bias-variance trade-off of these different techniques, particularly on large-scale PS models that can utilize up to $10^5 ~ 10^6$ covariates in the model. 
Much of the existing evidence is either based on simulations that may not fully capture the complexities of real-world data, or utilizing small-scale PS models with only 10-20 hand-picked covariates. 
This study aims to evaluate the performance of different PS adjustment strategies in a large-scale, real-world cohort of patients with type 2 diabetes mellitus (T2DM) initiating second-line treatment.

We specifically compare several PS adjustment methods, including 1-to-1 and 1-to-many matching, variable-ratio matching, stratification with 5, 10, and 20 groups, inverse probability of treatment weighting (IPTW), and overlap weighting.
In addition, we consider both large-scale propensity score (LSPS) models, utilizing OHDSI's standard sets of $10^5 ~ 10^6$ covariates and small-scale propensity score (SSPS) models, utilizing roughly 20-30 handpicked variables. 
To assess the bias-variance trade-offs of these techniques, we use a set of negative control outcomes, and a set of synthetic positive controls with predetermined effect sizes.
We aim to conduct a comprehensive evaluation of the trade-offs of utilizing different PS adjustment strategies.
The insights gained from this analysis on propensity score techniques are expected to guide decision-making in future studies utilizing observational healthcare data.

## Rationale and Background

The new-user cohort method attempts to emulate randomized clinical trials for observational data. 
Observational studies, however, may suffer from baseline differences between patient groups initiating different treatments that affect relative outcome risk estimation. [MAS: add citation?]{.aside}
One solution of interest is to estimate a propensity score (PS), the baseline probability that a patient will receive one treatment over the other, across both patient groups. 
The PS is modeled using a large-scale model of between 10,000 \~ 100,000 baseline patient characteristics. [MAS: please fix.  PS models are generally small, using which? specific covariates.  alternatively, they may be large-scale.  also add citations to "both" approaches]{.aside}
By matching, stratifying, or weighting patients based on their propensity scores, researchers aim to remove bias in the outcome risk estimate while achieving balance in the measured covariates between treatment groups.

Previous work indicates that propensity score matching at ratios of one-to-many $n$, with either fixed or variable values for $n$, can yield smaller variance in the eventual treatment effect estimation. [MAS: citation?  and smaller than what?]{.aside}
The resulting trade-off when going beyond one-to-one matching is increased bias: each patient's subsequent matches beyond their first may be less similar to them, and therefore, less optimal.
[@rassenOnetomanyPropensityScore2012] suggest variable ratio matching,
where each patient in the treated group is matched with a random number of patients in the control group, reduces bias compared to techniques with fixed ratios. [MAS: citation is not formatted correctly in-text]{.aside} [MAS: target and control need to be introduced before here.]{.aside}
This is due to the fact that variable ratio matching retains treated subjects who do not meet the criteria of fixed number matches (i.e. a treated subject only matches to one control, but fixed-ratio $1:3$ matching requires three matches to be included in the study). 
While the literature has existing recommendations and insights for the bias-variance trade-off of varying propensity score matching techniques, a large amount of its conclusions are drawn from simulations which have an unclear relationship with the real world, where residual systematic error persists. 
[MAS: you also need to introduce stratification and ALL other approaches (different weighing schemes) we are comparing.]{.aside}
[MAS: maybe also some discussion that different PS schemes leads to slightly different effect estimates, i.e. ATT, ATE, ATO, etc.]{.aside}

Here we seek to evaluate the bias-variance trade-offs of different PS adjustment techniques in a large-scale real world study across multiple data sources, in this case comparing the effectiveness and safety of initiating 2nd-line T2DM treatment across different drug classes.
Specifically, we focus on the options implemented for new-user cohort studies via `CohortMethod` from the OHDSI Methods Library. [MAS: `CohortMethod` comes from `HADES` (we no longer use OHDSI Methods Library) and HADES has a citation.  Fix here and elsewhere]{.aside}
[MAS: more importantly, do not use "options implemented" (that sounds lazy); instead enumerate the choices: matching with differing numbers of controls, stratifying, weighing, etc.]{.aside}
To measure performance, our benchmark examines a large set of negative control outcomes, where we believe the true relative treatment effect is 1, and also using imputed positive control outcomes with predetermined treatment effects > 1.

[MAS: please see Martijn's introduction in his benchmark study for a guide on how to draft your background; if you write it well now, that you save you much time when putting the manuscript / thesis together]{.aside}

## Study Objectives

### Research questions

How do various PS adjustment techniques, including stratifying, weighting, and multiple types of matching, affect the bias and variance of population-level effect estimation under the new-user cohort method?

### Objectives

To measure the bias-variance trade-off and other performance operating characteristics of the various propensity score adjustment methods in the OHDSI Methods Library using large-scale LEGEND-T2DM data. [MAS: what is new in this subsection compared to the "Research questions" about? ... please follow https://ohdsi-studies.github.io/LegendT2dm/Protocol.html#7_Study_Objectives]{.aside}

## Methods

### Study Design

#### CohortMethod

We plan to evaluate the new-user cohort method that attempts to emulate a randomized clinical trial using observational data [@hernanUsingBigData2016a]. 
Subjects who are observed to initiate a treatment of interest (the target) are compared to subjects initiating another treatment (the comparator) and are followed for a pre-specified amount of time following treatment initiation. 
One main difference between a randomized clinical trial and the cohort method is that there is no randomization of subjects between target and comparator, and hence patients receiving the target may systematically differ from patients receiving the comparator treatment. 
To adjust for confounding between target and comparator differences at initiation and outcome risk as a result of the lack of randomization, we considere here the use of propensity score models. 
The propensity score (PS) for any given patient is the unknown probability that they may receive the target treatment instead of the comparator, regardless of which treatment they actually received.
PS models are popularly built using logistic regression [MAS: citations?]{.aside} with debate continuing around which patient features known before treatment initation to include in the model.[MAS: LSPS citations, Yuxi, Lingying, maybe some in support of small models]{.aside}
and is computed by fitting a large-scale regularized [MAS: this is not true .. and are we not comparing small-scale vs large-scale?]{.aside}
[MAS: you need to describe feature choice for small scale models as well]{.aside}
[MAS: then describe LSPS, regularization (with Cyclops citations) and all the features]{.aside}
binary model (in this case, logistic regression) using a large subset of generic patient characteristics [@rosenbaumCentralRolePropensity1983; @austinIntroductionPropensityScore2011]. 
These generic characteristics may include items such as demographic information, medical diagnoses, drug exposures, and much more.

PS estimates are used to adjust for potential confouding in multiple ways, including: matching patients with similar scores between the target and comparator groups, stratifying the entire population of patients by score, or re-weighting patients using Inverse Probability of Treatment Weighing. 
Since propensity scores fall on a range of 0 to 1, exact matching or stratifying based on score is rarely possible. [MAS: i do not understand why a bounded-range implies exact matching or stratifying is not possible]{.aside}
@rosenbaumCentralRolePropensity1983;@cochranControllingBiasObservational1973 recommend a "caliper" of 0.2 standard deviations on the logit scale, where patients are able to be matched together if their scores fall within this given amount. 
There has been interest in different ways to match a patients initiating the target to those initiating the comparator. 
While patients could be directly matched 1-to-1, we could benefit from using more information by matching patients 1-to-many, where one target patient could be matched to $n \geq 1$ comparator. 
Fixed-ratio matching will find a specific $n$ such that each target attempts to match with $n$ comparators. 
Variable-ratio matching allows a variable number of comparators to be matched per target subjects.
[MAS: what about the different weighting schemes?]{.aside}

<!-- Previous studies indicate that one-to-many matching may increase the bias of the eventual outcome model's estimations with a little or negative effect on precision; meanwhile, variable-ratio matching increases the bias while simultaneously increasing the precision of the estimates [@]. -->

<!-- Another choice for adjusting for confounding is to include additional covariates in the outcome model. Typically, the outcome model with the propensity score is conditioned only on the propensity score and the treatment initiated. When outcomes are more rare, there may be a lack of data to fit an elaborate model. We can make the choice to add the same variables used in the PS model into the outcome model to adjust for the same variables two times in total, but with different methods. -->

#### Analysis settings

[MAS: describe the list of varying PS adjustment schemes FIRST, then SECOND describe the 2 * 6 ordered pairs for the 4 classes, then THIRD the negative controls]{.aside}

We will evaluate of an extensive list of varying PS adjustment schemes on pairwise comparisons for two second-line T2DM drug classes. [MAS: we want all 2 * 6 ordered pairs for the 4 classes?]{.aside}
For each analysis set, we employ an active comparator, new-user cohort design by selecting one target/comparator (exposure) pair and some outcomes of interest described in @sec-tcos. 

To measure the bias trade-off in the presence of residual systematic error, we utilize a large set of negative control outcome experiments. 
In the negative control outcomes, we assume true the null hypothesis of no effect, where we do not believe that exposure to the target or the comparator will cause the outcome. 
In this study, we utilize the same negative controls as those in the LEGEND-T2DM study, which validated approximately 100 negative controls [@kheraLargescaleEvidenceGeneration2022]. [MAS: Patrick added more negative controls into `Glp1Dili` that JnJ calls their "universal controls"; we should use all of these, and describe how they were generated.]{.aside}
These negative controls were generated using a data-rich algorithm that draws upon literature, product labels, and spontaneous reports. 

For each negative control, we further generate synthetic positive controls with target effect sizes of \[1.5, 2, 4\] by adding simulated outcomes to the existing negative control until the desired effect is reached [@colomaReferenceStandardEvaluation2013]. 

Our evaluation focuses on the differences in PS adjustment techniques applied to an outcome model and its included patients to estimate a relative effect.
@tbl-1 lists each variant to be evaluated. 

```{r}
baseline1 <- c("Unadjusted")
matching1 <- c("Matching (1:1)", "Matching (1:1)", "Matching (1:5)", "Matching (1:10)", "Matching (variable-ratio)")
strat1 <- c("Stratification (5 groups)", "Stratification (10 groups)", "Stratification (20 groups)")
weight1 <- c("IPTW", "Overlap weighing")

lsps <- tibble(method = c(matching1, strat1, weight1),
               stratified = "Y")

lsps$stratified[c(1, 9:10)] <- "N"
lsps$model <- "LSPS"
lsps$id <- c(10:19)

# lsps$model[1] <- "None"

# matching2 <- c("Matching (1:1)", "Matching (1:5)", "Matching (variable-ratio)")
# strat2 <- c("Stratification (5 groups)")
# weight2 <- c("IPTW")
# 
# ssps <- tibble(method = c(matching2, strat2, weight2),
#                stratified = "Y")
# ssps$stratified <- "N"
# ssps$stratified[3] <- "Y"
ssps <- lsps
ssps$model <- "SSPS"
ssps$id <- c(20:29)

base <- tibble(id = 1, method = "Unadjusted",
               stratified = "N",
               model = "None")

data1 <- rbind(base, lsps, ssps)

table1 <- kbl(data1,
              booktabs = T,
              col.names = c("ID",
                            "PS adjustment variant",
                            "Conditional outcome model",
                            "PS model type"),
              align = "rlcc",
              linesep = "") %>% 
    kable_styling(latex_options = "striped",
                full_width = F) 

```

```{r}
#| label: tbl-1
#| tbl-cap: "New-user cohort method analysis variants of propensity score (PS) adjustment of outcome model, its conditioning and patient usage. DEFINE LSPS, SSPS, etc. here"
table1
```

Each variant assumes a popular Cox proportional hazards approach to model time-to-event until first outcome or censuring after treatment initiation with a single treatment-indicator covariate.
Variants further condition (or not) the Cox model to have independent baseline hazard functions across matched-set and strata.
When matching PS scores, we use a recommended caliper of 0.2 standard deviations on the standardized logit scale [@cochranControllingBiasObservational1973].
[MAS: insert what the 5 or 10 or 20 means for stratification. based on quantitles?]{.aside}
When performing IPTW, we trim [MAS: at what values?]{.aside} to counter the effect of extreme weights on the model. 

Finally, variants consider both large-scale propensity score (LSPS) and small-scale propensity score (SSPS) models.
LSPS models utilize a data-driven approach to covariates, which selects a large subset of patient characteristics. [MAS: you can provide a better description by taking text for recent papers]{.aside}
<!-- Question for Marc: Should I include list of things included in the PS model (i.e., drug_era 365 days...)? I don't see it in the LEGEND protocol : YES --> 
Our SSPS models include roughly $50$ hand-picked covariates. [MAS: how? why where those picked?]{.aside} 
Existing literature often uses smaller models with handpicked variables, so we aim to generate a comparable small-scale model using OHDSI's toolstack. 
@sec-ssps lists the chosen covariates of interest. 
<!-- TODO?: frequency / characterization for the small-scale covariates, like in Graham's paper : at least a description of _why_ these were picked -->

### Data sources

We will evaluate all PS variants for all comparisons across a series of administraitive claims and electronic health records (EHR) data through the OHDSI Evidence Network. 
The models are conveniently run across differing database types as a result of the community's shared Observational Medical Outcomes Partnership (OMOP) common data model (CDM) and the OHDSI Methods Library.

@tbl-datasources lists the 5 data sources that will be utilized in this evaluation, with a brief description and population size, along with information on the data capturing process for its patients and its start date.

```{r}
#| label: tbl-datasources
#| tbl-cap: "Committed data sources and their covered populations"

data_sources <- readr::read_delim(col_names = TRUE, delim = ";", trim_ws = TRUE, file = "
  Data source ; Population ; Patients ; History ; Data capture process and short description
  IBM MarketScan Commercial Claims and Encounters (CCAE) ; Commercially insured, < 65 years ; 142M ; 2000 -- ; Adjudicated health insurance claims (e.g. inpatient, outpatient, and outpatient pharmacy)  from large employers and health plans who provide private healthcare coverage to employees, their spouses and dependents.
  IBM MarketScan Medicare Supplemental Database (MDCR)  ; Commercially insured, 65+ years ; 10M ; 2000 -- ; Adjudicated health insurance claims of retirees with primary or Medicare supplemental coverage through privately insured fee-for-service, point-of-service or capitated health plans.
  IBM MarketScan Multi-State Medicaid Database (MDCD) ; Medicaid enrollees, racially diverse ; 26M ; 2006 -- ; Adjudicated health insurance claims for Medicaid enrollees from multiple states and includes hospital discharge diagnoses, outpatient diagnoses and procedures, and outpatient pharmacy claims.
  Optum Electronic Health Records (OptumEHR) ; US, general ; 93M ; 2006 -- ; Clinical information, prescriptions, lab results, vital signs, body measurements, diagnoses and procedures derived from clinical notes using natural language processing.
",
show_col_types = FALSE)
tab <- kable(data_sources, booktabs = TRUE, linesep = "") %>%
  kable_styling(bootstrap_options = "striped", latex_options = "striped") %>%
  pack_rows("Administrative claims", 1, 3, latex_align = "c", indent = FALSE) %>%
  pack_rows("Electronic health records (EHRs)", 4, 5, latex_align = "c", indent = FALSE)

if (knitr::is_latex_output()) {
  tab %>%
    column_spec(1, width = "6em") %>%
    column_spec(2, width = "6em") %>%
    column_spec(5, width = "20em")
} else {
  tab
}

```

### Target, comparator, and outcome specifications {#sec-tcos}

#### Exposure comparators

We utilize four exposure cohorts for new-users of any drug ingredient wihin the four traditionally second-line drug classes for type-2-diabetes mellitus (T2DM) treatment: DPP4, GLP1, SGLT2, and SU. <!-- The index date for each patient is their first observed exposure to any drug ingredient for the four second-line drug classes.  --> 
@sec-cohortdefs reports the complete OHDSI `ATLAS` cohort descriptions for new-users of each drug class. 
The description includes complete specification of cohort entry events, inclusion criteria, cohort exit events, and all associated OMOP-CDM concept codes used in the definition of the cohort. 
For each data source presented in @tbl-datasources, we then execute pairwise class comparisons, where in each pair, each class has one opportunity to serve as the target and the other as the comparator. This results in $C \binom{4}{2} * 2 = 12$ combinations to execute.

```{r}
#| label: tbl-cohortcounts
#| tbl-cap: "Cohort counts for exposure comparators across datasources"

counts <- readRDS("./figures/cohortCounts.rds")
kbl(counts,
              booktabs = T,
              col.names = c("Cohort name",
                            "Cohort ID",
                            "CCAE",
                            "MDCD",
                            "MDCR",
                            "OptumEHR"),
              align = "llcccc",
              linesep = "") %>% 
    kable_styling(latex_options = "striped",
                full_width = F) 

```

@tbl-cohortcounts displays the cohort counts for each of the second-line drug class comparators.

#### Outcomes

The chosen negative control concepts are reported in @sec-ncs. 
For each negative control, synthetic outcomes with expected hazard ratio $[1.5, 2, 4]$ are generated to act as positive controls.

<!-- The three main outcomes of interest are defined using manually crafted rule-based definitions using a combination of diagnosis concept codes in the OMOP-CDM: -->

<!-- -   3-point major adverse cardiovascular events (MACE), including acute myocardial infarction, stroke, and sudden cardiac death, and -->

<!-- -   4-point MACE that additionally includes heart failure hospitalization. -->

<!-- -   Acute renal failure -->

<!-- These outcome definitions have been previously implemented and validated in the `LEGEND-T2DM` study \[\@?\]. The formal `ATLAS` definitions of the outcomes are found below. -->

### Metrics
Each propensity score adjustment method will be applied to one analysis estimating the treatment effects on the negative and positive controls.
We assume that each analysis will produce an effect size estimate for each control, uncertainty of the estimate expressed as a 95% confidence interval, and a two-sided p-value for the null hypothesis of no effect.
Prior work shows that the p-value and confidence intervals can be empirically calibrated to restore their operating characteristics to near-nominal values using the observed distribution of estimates for the negative and positive controls [@schuemieEmpiricalConfidenceInterval2018;@schuemieInterpretingObservationalStudies2014].

We will apply empirical calibration before computing and reporting the performance metrics adopted from previous research [@schuemieHowConfidentAre2020]:

  - Coverage: How often is the true effect size within the 95% confidence interval for each control?
  - Non-estimable: How many controls was the method unable to produce an estimate? This can be for many reasons, e.g. Not enough subjects left after PS adjustment.
  - Mean precision: computed as $\frac{1}{SE^2}$, where higher precision implies lower variance and narrower confidence intervals.
  - Mean squared error (MSE): mean squared difference between the estimated effect size and the true effect size
  - Type I error: For negative controls, how often was the null rejected ($\alpha = .05$)? This is equivalent to the false positive rate and 1 - specificity.
  - Type II error: For positive controls, how often was the null not rejected? ($\alpha = .05$)? This is equivalent to the false negative rate and 1 - sensitivity.
  - Sensitivity and specificity according to the previous definitions for Type I / II error.
  - AUC: How well can we discriminate between positive and negative controls?

The metrics will be computed overall, and also stratified by the true effect size, especially for the positive controls.
We will evaluate each adjustment method primarily for its bias-variance trade-off by examining the MSE and precision of the treatment effect estimates, before we take into consideration the other performance metrics.

<!-- ## Sample Size and Study Power -->

<!-- <!-- from MethodEvaluation: Within each database, the minimum detectable relative risk (MDRR) will be computed for each control as a typical proxy for power.  --> -->

<!-- We inspect the distributions of the PS estimates to evaluate population generalizability and cohort balance before and after adjustment.  -->
<!-- Finally, Kaplan-Meier plots are utilized to examine hazard ratio proportionality assumptions. [MAS: these two sentences have nothing to do with sample size or power]{.aside} -->

## Strengths and Limitations

### Strengths

We aim to conduct an empirical evaluation of a wide combination of propensity score adjustment methods to population-level estimation across databases containing large-scale administrative claims and EHR data. 
The existence of a large set of real-world negative controls and synthetic positive controls allow for quantification of unmeasured and systemic bias inherent in observational studies.

### Limitations

While the study uses a wide range of US health data, some operating characteristics will depend on the choice of database, and hence generalization may be difficult for databases outside of this study. 
In addition, the use of real-world negative control outcomes does not imply that the true confounding structure between treatments and outcomes is known. 
We utilize a large set of negative controls that represent a wide range of confounding structures, such that one method leading to reduced bias and higher precision for many structures implies those positive effects on the eventual effect estimates. 
On the other hand, synthetic positive controls only reflect measured bias, due to the use of injected outcomes based on negative controls. 
Finally, the use of real-world data contains certain issues, including limited and variable observed follow-up times, and missing visit/care episodes for patients. 
We believe such bias from using a real-world data source will likely be towards the null.

## Protection of Human Subjects

This study uses human data collected during routine healthcare provision, and all data is de-identified within the data source. 
All data partners executing the analyses within their data sources will have received institutional review board (IRB) approval or waiver for participation in accordance to their institutional governance prior to execution.
PME executes across a federated and distributed data network, where participating data partners only receive analysis code and return aggregate summary statistics, with no sharing of patient-level data between organizations.

<!-- todo: table 10.1, per EUMAEUS / BETTER protocol -->


## Sample results table

```{r}
#| label: tbl-metricsPreCali
#| tbl-cap: "Performance metrics before empirical calibration across all positive and negative controls"
metrics <- readRDS("./figures/metricsPreCali.rds")

metrics %>%
  mutate(
    auc = color_bar("lightpink")(auc),
    coverage = color_bar("lightpink")(coverage),
    meanP = color_bar("lightpink")(meanP),
    mse = color_bar("lightblue")(mse),
    type1 =color_bar("lightblue")(type1),
    type2 = color_bar("lightblue")(type2),
    nonEstimable = color_bar("lightgreen")(nonEstimable)
  ) %>%
  select(analysisDescription, auc, coverage, meanP,
         mse, type1, type2, nonEstimable) %>%
  kable(escape = F,
        booktabs = T,
        col.names = c("Description", "AUC", "Coverage", "Mean Precision",
                      "MSE", "Type-I Error", "Type-II Error", "Non-estimable (%)"))
```

```{r}
#| label: tbl-metricsPostCali
#| tbl-cap: "Performance metrics after empirical calibration for SGTL2is vs GLP1s across all positive and negative controls"
metricsCali <- readRDS("./figures/metricsCali.rds")
metricsCali %>%
  mutate(
    auc = color_bar("lightpink")(auc),
    coverage = color_bar("lightpink")(coverage),
    meanP = color_bar("lightpink")(meanP),
    mse = color_bar("lightblue")(mse),
    type1 =color_bar("lightblue")(type1),
    type2 = color_bar("lightblue")(type2),
    nonEstimable = color_bar("lightgreen")(nonEstimable)
  ) %>%
  select(analysisDescription, auc, coverage, meanP,
         mse, type1, type2, nonEstimable) %>%
  kable(escape = F,
        booktabs = T,
        col.names = c("Description", "AUC", "Coverage", "Mean Precision",
                      "MSE", "Type-I Error", "Type-II Error", "Non-estimable (%)"))
```

## Plans for Disseminating and Communicating Study Results

Open science aims to make scientific research, including its data processing, software, and dissemination, accessible to all levels of an inquiring society [@woelfleOpenScienceResearch2011].
Open science delivers reproducible, transparent, and reliable evidence.
All aspects of PME (except private patient data) will be open and we will actively encourage other interested researchers, clinicians, and patients to participate.
This differs fundamentally from traditional studies that rarely open their analytic tools or share all result artifacts, informing the community about hard-to-verify conclusions at completion.

### Transparent and re-usable research tools
We plan to publicly register this protocol and announce its availability for feedback from stakeholders, the OHDSI community, and within clinical professional societies.
This protocol will link to open source code for all the steps utilized to generate diagnostic measurements, effect estimates, figures, and tables. 
Such transparency is possible because we construct our studies on top of the OHDSI toolstack of open source software tools that are community developed and rigorously tested [@Schuemie2020How].
We will publicly host PME source code at *github.com/pme*, allowing public contribution and review, and free re-use for anybody's future research.

### Scientifc meetings and publications
We will deliver multiple presentations at scientific venues and will also prepare publications for clinical, informatics, and statistical journals.

### General public
We believe in sharing our findings that will guide observational health data research with the general public.
With dedicated support from the OHDSI communications specialist, we will deliver regular press releases at key project stages, distributed via the extensive media networks of UCLA and Columbia.

## Appendix

### Cohort definitions {#sec-cohortdefs}

The following is an example class cohort definition for DPP4-inhibitors. 
Cohort definitions for other classes are very similar.

```{r,  echo=FALSE, results="asis", warning=FALSE, message=FALSE}
source("./PrettyOutput.R")
printCohortDefinitionFromNameAndJson(name = "New-users of DDP4Is",
                                     json = SqlRender::readSql("../inst/cohorts/ID101100000.json"),
                                     withConcepts = TRUE)
```

### Small-scale propensity model {#sec-ssps}

All covariates for condition or drug use are defined as those from 365 days prior to index date.

```{r}
ssCovs <- readRDS("./figures/sspsRef.rds") %>%
  arrange(covariateId)

ssCovs$covariateName <- gsub(".*index: ", "", ssCovs$covariateName)

kbl(ssCovs,
    booktabs = T,
    col.names = c("Covariate ID",
                  "Covariate name",
                  "Analysis ID")) %>%
  kable_styling(latex_options = "striped",
                full_width = F)
```

### Negative control concepts {#sec-ncs}

```{r}
ncs <- read.csv("../inst/settings/NegativeControls.csv") %>%
  select(outcomeId, outcomeName)

kbl(ncs,
    booktabs = T,
    col.names = c("Outcome ID",
                  "Name")) %>%
  kable_styling(latex_options = "striped",
                full_width = F)
```
